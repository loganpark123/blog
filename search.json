[
  {
    "objectID": "oss/opensource.html",
    "href": "oss/opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "My open soruce work has been focused on developer tools and infrastructure. I‚Äôve contributed to projects such as fastai, Metaflow, Kubeflow, Jupyter, and Great Expectations, as well as many others. I list some of these below:"
  },
  {
    "objectID": "oss/opensource.html#fastai",
    "href": "oss/opensource.html#fastai",
    "title": " Open Source",
    "section": " fastai",
    "text": "fastai\nI maintain and contribute to a variety of fastai projects. Below are the projects I‚Äôve been very involved in:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nfastpages \n\n\nAn easy to use blogging platform for Jupyter Notebooks. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nnbdev \n\n\nWrite, test, document, and distribute software packages and technical articles all in one place, your notebook. \n\n\nCore Contributor\n\n\nBlog, Talk\n\n\n\n\nfastcore \n\n\nA Python language extension for exploratory and literate programming. \n\n\nCore Contributor\n\n\nBlog\n\n\n\n\nghapi \n\n\nA Python client for the GitHub API \n\n\nCore Contributor\n\n\n Blog\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#metaflow",
    "href": "oss/opensource.html#metaflow",
    "title": " Open Source",
    "section": " Metaflow",
    "text": "Metaflow\nI created notebook cards: A tool that allows you to use notebooks to generate reports, visualizations and diagnostics in Metaflow production workflows. Blog"
  },
  {
    "objectID": "oss/opensource.html#kubeflow",
    "href": "oss/opensource.html#kubeflow",
    "title": " Open Source",
    "section": " Kubeflow",
    "text": "Kubeflow\nI‚Äôve worked on several projects related to Kubeflow, mainly around examples and documentation:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nGitHub Issue Summarization\n\n\nAn end-to-end example of using Kubeflow to summarize GitHub Issues. Became one of the most popular tutorials of Kubeflow. \n\n\nAuthor\n\n\nInterview with Jeremy Lewi\n\n\n\n\nkubeflow/codei-intelligence\n\n\nVarious tutorials and applied examples of Kubeflow. \n\n\nCore Contributor\n\n\nTalk\n\n\n\n\nThe Kubeflow Blog\n\n\nI used fastpages to create the official Kubeflow blog. \n\n\nCore Contributor\n\n\nSite\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#jupyter",
    "href": "oss/opensource.html#jupyter",
    "title": " Open Source",
    "section": " Jupyter",
    "text": "Jupyter\nI created the Repo2Docker GitHub Action, which allows you to trigger repo2docker to build a Jupyter enabled Docker images from your GitHub repository. This Action allows you to pre-cache images for your own BinderHub cluster or for mybinder.org.\nThis project was accepted into the official JupyterHub GitHub org."
  },
  {
    "objectID": "oss/opensource.html#great-expectations",
    "href": "oss/opensource.html#great-expectations",
    "title": " Open Source",
    "section": " Great Expectations",
    "text": "Great Expectations\nI developed the Great Expectations GitHub Action that allows you to use Great Expectations in CI/CD Workflows. Blog."
  },
  {
    "objectID": "oss/opensource.html#other",
    "href": "oss/opensource.html#other",
    "title": " Open Source",
    "section": " Other",
    "text": "Other\nI worked as a staff machine learning engineer at GitHub from 2017 - 2022. I led or created the following open source projects that explored the intersection of machine learning, data and the developer workflow:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nCode Search Net \n\n\nDatasets, tools, and benchmarks for representation learning of code. This was a big part of the inspiration for GitHub‚Äôs eventual work on CoPilot. \n\n\nLead\n\n\n Blog, Paper\n\n\n\n\nMachine Learning Ops\n\n\nA collection of resources on how to facilitate Machine Learning Ops with GitHub. This project explored integrations with a wide variety of data science tools with GitHub Actions. \n\n\nCreator\n\n\nBlog\n\n\n\n\nIssue Label Bot\n\n\nA GitHub App powered by machine learning that auto-labels issues. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nCovid19-dashboard \n\n\nA demonstration of how to use GitHub Actions, Jupyter Notebooks and fastpages to create interactive dashboards that update daily.\n\n\n\nCreator\n\n\nNews Article\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/cuda.html",
    "href": "notes/cuda.html",
    "title": "CUDA Version Management",
    "section": "",
    "text": "There are many libraries that only support specific versions of CUDA. Downgrading/upgrading CUDA can sometimes be tricky (especially downgrading). It‚Äôs often desirable to manage CUDA versions per project (instead of globally), without having to reach for Docker."
  },
  {
    "objectID": "notes/cuda.html#problem",
    "href": "notes/cuda.html#problem",
    "title": "CUDA Version Management",
    "section": "",
    "text": "There are many libraries that only support specific versions of CUDA. Downgrading/upgrading CUDA can sometimes be tricky (especially downgrading). It‚Äôs often desirable to manage CUDA versions per project (instead of globally), without having to reach for Docker."
  },
  {
    "objectID": "notes/cuda.html#solution",
    "href": "notes/cuda.html#solution",
    "title": "CUDA Version Management",
    "section": "Solution",
    "text": "Solution\nYou can use conda to manage your CUDA versions! This allows you to isolate specific CUDA versions to specific environments rather than managing CUDA versions globally.\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm using mamba which has faster solvers than conda. Refer to the docs for installation instructions.\n\n\nLet‚Äôs say I want to downgrade to CUDA 11.7 in its own conda environment. First, I will create a new environment named cuda11-7 with the following command:\nmamba create -n cuda11-7 python=3.8\nmamba activate cuda11-7\nBefore I downgrade, we can check our CUDA version with the following command:\n&gt; nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nAs you can see, I have CUDA version 11.8 but I want to downgrade to 11.7. We can downgrade CUDA by using cuda-toolkit:\nmamba install -c \"nvidia/label/cuda-11.7.1\" cuda-toolkit\nThis will take several minutes to complete. Next, recheck your CUDA version:\n&gt; nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\nCuda compilation tools, release 11.7, V11.7.99\nBuild cuda_11.7.r11.7/compiler.31442593_0\nNext, you need to install the correct version of PyTorch for your CUDA version. It is crucial to install the right version of PyTorch that matches your CUDA version exactly. For example, if you want to install PyTorch with CUDA 11.7, you can use the following command:\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\nYou can find PyTorch installation instructions on this page.\nViola! You have downgraded your CUDA version successfully. Note that this version of CUDA is isolated to this specific environment.\nTo make sure that everything is working correctly, make sure you can import torch and check the CUDA version from within Python:\n&gt; python -c \"import torch; print(torch.version.cuda)\"\n11.7"
  },
  {
    "objectID": "notes/cuda.html#additional-resources",
    "href": "notes/cuda.html#additional-resources",
    "title": "CUDA Version Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhy does nvcc --version sometimes report a different CUDA version than nvidia-smi? See this answer on Stack Overflow.\nTwitter discussion on this topic."
  },
  {
    "objectID": "guest-blog.html",
    "href": "guest-blog.html",
    "title": "Guest Blogs",
    "section": "",
    "text": "nbdev + Quarto: A new secret weapon for productivity, the fastai blog, July 2022.\nNotebooks in production with Metaflow Introduces a new Metaflow feature that allows users to use notebooks in production ML workflows.\nPython Concurrency: The Tricky Bits: An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each.\nghapi, a new third-party Python client for the GitHub API by Jeremy Howard & Hamel Husain, GitHub Repo.\nNbdev: A literate programming environment that democratizes software engineering best practices by Hamel Husain, Jeremy Howard, The GitHub Blog.\nfastcore: An Underrated Python Library by Hamel Husain, Jeremy Howard, GitHub Repo.\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes: An end-to-end example of deploying a machine learning product using Jupyter, Papermill, Tekton, GitOps and Kubeflow. by Jeremy Lewi, Hamel_Husain, The Kubeflow Blog.\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. by Jeremy Howard & Hamel Husain, GitHub Repo\nGitHub Actions: Providing Data Scientists With New Superpowers by Jeremy Howard & Hamel Husain.\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search: by Miltiadis Allamanis, Marc Brockschmidt, Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit GitHub Repo\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning. (Related: GitHub engineering blog article, Live demo)\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\nHow Docker Can Make You A More Effective Data Scientist\nAutomated Machine Learning, A Paradigm Shift That Accelerates Data Scientst Productivity At Airbnb"
  },
  {
    "objectID": "blog/posts/k8s/index.html",
    "href": "blog/posts/k8s/index.html",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "",
    "text": "K8s For Data Scientists Course\n\n\n\nIf you came here looking for the course, feel free to jump ahead to: K8s For Data Scientists.\nKubernetes, known as K8s, is an open-source system for deploying and managing containerized applications in the cloud. An increasing amount of modern web applications are deployed on K8s. If you are an ML engineer, it is increasingly likely that either the infrastructure you use to train, monitor, or orchestrate your models is deployed on K8s, or downstream applications that consume your models are running on K8s. However, K8s is a complex system that can be intimidating to learn.\nI agree with Chip Huyen that, in theory, Data Scientists shouldn‚Äôt need to learn K8s. However, the truth is: Even though you shouldn‚Äôt have to, it‚Äôs really beneficial if you do! I‚Äôve found that I‚Äôm often constrained by infrastructure and that infrastructure is increasingly hosted on Kubernetes.\nFor example, I‚Äôm rarely given access to a cloud provider‚Äôs console, and instead, I have access to a K8s cluster with some data tools already installed. When something goes awry, it‚Äôs beneficial to know enough about K8s to debug the issue. Additionally, familiarity with basic concepts allows me to have more productive conversations with my team about infrastructure.\nVicki Boykis seems to agree that the investment in learning this technology is worthwhile1:\nBelow, I outline several reasons why learning K8s is a good idea for machine learning engineers2."
  },
  {
    "objectID": "blog/posts/k8s/index.html#hosted-dataml-tools-are-not-always-an-option",
    "href": "blog/posts/k8s/index.html#hosted-dataml-tools-are-not-always-an-option",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Hosted data/ML tools are not always an option",
    "text": "Hosted data/ML tools are not always an option\n\n\n\nA robot concierge helping a scientist\n\n\nLarge cloud providers offer their flavors of ML infrastructure as hosted solutions3. However, there is often a gap between these offerings and the needs of machine learning teams. For example, I‚Äôve seen the following tools deployed alongside or in place of hosted solutions:\n\nMetaflow\nKubeflow\nArgo\nJupyterHub\nDask\netc.\n\nWhen open source isn‚Äôt enough, third-party vendors are happy to install their software on your cloud. However, you often need basic infrastructure skills to enable this. These skills often intersect with Kubernetes. While you may not be responsible for deploying the infrastructure yourself, it is helpful to understand the basics of how things work so that you can do basic debugging and troubleshooting. For example, knowing where to find logs or an API/HTTPS endpoint can unblock you in many cases."
  },
  {
    "objectID": "blog/posts/k8s/index.html#nobody-is-coming-to-save-you",
    "href": "blog/posts/k8s/index.html#nobody-is-coming-to-save-you",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Nobody is coming to save you",
    "text": "Nobody is coming to save you\n\n\n\nA super hero\n\n\nA typical first experience as a machine learning professional is that you don‚Äôt have the necessary tools to get started. This is incredibly frustrating, as making progress without the proper tools can be hard. This experience usually culminates in a conversation like this:\n\nML Eng: I‚Äôm excited to join ACME company! You‚Äôve hired me to optimize marketing spending with predictive models. The issue is that we don‚Äôt have the basic infrastructure or tools necessary for me to work efficiently.\nManager: I‚Äôm confused. Can‚Äôt you install the tools you need? Isn‚Äôt that what you are for? I was expecting that you would figure it out.\nML Eng: No, I don‚Äôt know how to set up and deploy infrastructure. We need a special infrastructure or DevOps person for that.\nManager: It will be hard to ask for more resources if we don‚Äôt know the expected return on investment. Can you do the ML project first, demonstrate some value, and then we can invest in infrastructure?\nML Eng: I need some minimum tools to experiment more quickly and develop a proof of concept. Also, I need tools that might help me collaborate better with my team‚Ä¶\n\nMy experience is that DevOps teams are chronically understaffed and overworked. While it usually isn‚Äôt advisable to deploy enterprise software yourself on Kubernetes for security concerns, having basic skills can lift a tremendous burden off your DevOps counterparts and make it tractable for them to help you.\nK8s are not a panacea for all infrastructure problems. You must operate within the constraints of your organization and existing software stack.4 However, with its growing popularity, it is increasingly likely that learning this technology will help you."
  },
  {
    "objectID": "blog/posts/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "href": "blog/posts/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "ML research is crowded. Compete on SWE skills.",
    "text": "ML research is crowded. Compete on SWE skills.\n\n\n\nAn overcrowded room of scientists\n\n\nOne of the best ways to set yourself apart as a data scientist is through your skills. Traditional education often emphasizes learning the latest ML techniques. However, cutting-edge ML research is very competitive. It‚Äôs also an extremely crowded space.\nIn my experience, the bottleneck many teams face is not a lack of knowledge of cutting-edge ML techniques but software engineering skills and partners to help operationalize models. If you take some time to learn how to stand up tools and infrastructure, you will be invaluable to your team.\nMore importantly, deploying and integrating models into services and applications is critical to connecting ML to business problems. Learning K8s will help you do this."
  },
  {
    "objectID": "blog/posts/k8s/index.html#your-company-likely-already-runs-k8s",
    "href": "blog/posts/k8s/index.html#your-company-likely-already-runs-k8s",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Your company likely already runs K8s",
    "text": "Your company likely already runs K8s\n\n\n\nA scientist shaking hands with someone who runs infrastructure\n\n\nJust as Python is the lingua franca of data science, K8s is becoming the lingua franca of cloud infrastructure. According to a 2021 Survey by CNCF, 96% of organizations are either using or evaluating Kubernetes. Furthermore, Stack Overflow‚Äôs 2022 Developer Survey shows that Docker and Kubernetes are the number one and two most loved and wanted tools, respectively. This is a strong indicator that K8s are here to stay.\nBasic proficiency with K8s will drastically increase your chances of garnering support for your desired tools in many organizations. Proficiency with K8s increases the likelihood that:\n\nYour DevOps counterparts will feel comfortable with the tools you want to deploy\nYou will have a shared language in which to talk to your application administrators\nYou will be more likely to attract people to help you with infra 5\n\nThese factors make it much more likely that you will get the tools that meet you where you are as opposed to something a software engineer without any data science experience thinks is a good idea (which I‚Äôve seen happen a lot!)."
  },
  {
    "objectID": "blog/posts/k8s/index.html#but-isnt-it-overkill",
    "href": "blog/posts/k8s/index.html#but-isnt-it-overkill",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "But isn‚Äôt it overkill?",
    "text": "But isn‚Äôt it overkill?\n\n\n\nCutting oranges with a chainsaw\n\n\nFor simple apps that you want to stand up quickly or prototype, K8s is overkill. Instead, I‚Äôm advocating knowledge of K8s as useful when working within the environments found in many companies. For example, hosting your data product on a single VM is often insufficient if you want to deploy production software. Many companies even have infrastructure that may block you from doing this with paved paths that only include Kubernetes.\nEven if you are not deploying any production software, K8s can be invaluable in allowing you to deploy the tools you need. In many cases using K8s can make tasks easier. Enterprises have necessarily invested resources in creating guardrails to control costs and security. Those guardrails are increasingly built around K8s patterns6. Understanding these concepts can make operating within the confines of your company‚Äôs cloud stack easier."
  },
  {
    "objectID": "blog/posts/k8s/index.html#you-dont-need-to-be-an-expert",
    "href": "blog/posts/k8s/index.html#you-dont-need-to-be-an-expert",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "You don‚Äôt need to be an expert",
    "text": "You don‚Äôt need to be an expert\n\n\n\nA student sitting at a desk in a library\n\n\nK8s are complicated, but you don‚Äôt need to become an expert to unlock great value as a Data Scientist. I‚Äôm not suggesting that data scientists become K8s administrators. K8s Administration is a very involved task and worthy of its own role. Unfortunately, nearly all educational material around K8s is focused on being an administrator, which is overkill for what most data scientists need."
  },
  {
    "objectID": "blog/posts/k8s/index.html#a-course",
    "href": "blog/posts/k8s/index.html#a-course",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "A course?",
    "text": "A course?\nI haven‚Äôt yet found a good resource for people like data scientists to learn Kubernetes without wading through lots of irrelevant material geared towards administrators. So my colleagues and I are considering creating a free course with data scientists in mind. If this sounds interesting, you can sign up here."
  },
  {
    "objectID": "blog/posts/k8s/index.html#footnotes",
    "href": "blog/posts/k8s/index.html#footnotes",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVicki is not someone who is impressed by flashy or new technologies and is someone who takes a pragmatic approach to get the job done. When she says you should learn K8s, you should pay attention!‚Ü©Ô∏é\nEach subsection of this article has a picture that has been generated by Stable diffusion with a prompt that very similar to the image caption.‚Ü©Ô∏é\nThese systems are AWS - Sagemaker, Azure - AzureML and GCP - VertexAI.‚Ü©Ô∏é\nSome organizations have built solutions that avoid K8s. For example, BigHat uses a solution based on AWS SageMaker + Lambda and other hosted solutions. So it might be a mistake to try to move over to K8s in that example ‚Äì you should try to leverage your company‚Äôs existing tech stack where possible!‚Ü©Ô∏é\nMy friend Micha≈Ç Jastrzƒôbski, who specializes in ML infrastructure, has shared the following colorful anecdote with me: ‚Äúwhen I hear Data Scientists shouldn‚Äôt learn K8s‚Äù, I hear ‚ÄúDevOps needs to learn Airflow‚Äù.‚Ü©Ô∏é\nSpecifically, K8s concepts that are relevant are namespaces, labels and RBAC.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logans's Blog",
    "section": "",
    "text": "Hello, I‚Äôm Logan Parker. I‚Äôm a machine learning engineer who loves building machine learning infrastructure and tools üë∑üèº‚Äç‚ôÇÔ∏è. I currently work at CoreLogic primarily focusing on Computer Vision (CV) and 3D reconstruction. Furthermore, I have experience (2+ years) as a software engineer across various industries.\nI‚Äôm currently exploring entrepreneurship at the intersection of dev tools, ML platforms and large language models. I am working on some projects right now including a proper solution to pesky bill splitting."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Logans's Blog",
    "section": "üíº Get In Touch",
    "text": "üíº Get In Touch\nDo you need help operationalizing ML or large language models or maybe want to chat about VR research?\nEmail me at logansparker@gmail.com if you‚Äôd like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Logans's Blog",
    "section": "üìÆ Blog Posts",
    "text": "üìÆ Blog Posts\nSubscribe here\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n\n\n5/30/23\n\n\nOn commercializing nbdev\n\n\nWhy I decided not to commercialize nbdev.\n\n\n\n\n\n\n\n1/16/23\n\n\nWhy Should ML Engineers Learn Kubernetes?\n\n\nLearning K8s can give you an unreasonable advantage as an MLE and unblock your team.\n\n\n\n\n\n\n\n7/28/22\n\n\nnbdev + Quarto: A new secret weapon for productivity\n\n\nOur favorite tool for software engineering productivity‚Äìnbdev, now re-written with Quarto. \n\n\nundefined\n\n\n\n\n2/9/22\n\n\nNotebooks in production with Metaflow\n\n\nIntroduces a new Metaflow feature that allows users to use notebooks in production ML workflows. \n\n\nundefined\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "üé§ Talks",
    "section": "",
    "text": "These are a list of talks I‚Äôve given:\n\nInnovating on Software Development, Data Council, March 2023.\nAutoML, Literate Programming, and Data Tooling Cargo Cults, Vanishing Gradients Podcast with Hugo Bowne Anderson, July 2022."
  },
  {
    "objectID": "blog/posts/nbdev/index.html",
    "href": "blog/posts/nbdev/index.html",
    "title": "On commercializing nbdev",
    "section": "",
    "text": "nbdev is a software development tool based on Jupyter that feels like its from the future.\nA few friends have asked me why I decided not to commercialize nbdev, especially after putting lots of work into the project, including leaving my full-time job to work on it. So I thought I would write a short post to explain my reasoning."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#background",
    "href": "blog/posts/nbdev/index.html#background",
    "title": "On commercializing nbdev",
    "section": "Background",
    "text": "Background\nnbdev is an innovative software development framework for Python that embraces literate and exploratory programming. I worked on nbdev from 2020-2023 with Jeremy Howard and, later, Wasim Lorgat. I had the privilege and excitement of exploring the boundaries of developer tools and exploratory programming while working with very talented software engineers. In addition to creating a tool many people enjoyed, I enjoyed using nbdev for personal and professional projects."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#opportunities",
    "href": "blog/posts/nbdev/index.html#opportunities",
    "title": "On commercializing nbdev",
    "section": "Opportunities",
    "text": "Opportunities\nWhile conducting product research, I interviewed many developers from different backgrounds to understand their pain points and needs. All developers I talked to struggled with one key challenge: it was difficult, if not impossible, to convince other engineers to use nbdev.\nThe following are the biggest reasons that prevented adoption:\n\nFriction in onboarding engineers. In many companies, there are often existing Python projects, and it can be detrimental to maintain different ways of doing things when a company has already settled upon one way that it has built processes and tools around.\nCollisions with the rest of the software development stack: it was (and still is) a pain to version control notebooks in a way that‚Äôs conducive to collaboration. For practical purposes, you cannot perform code reviews of notebooks on GitHub without purchasing a tool called ReviewNB. So instead of convincing people to use nbdev, you have to convince them to use nbdev and ReviewNB. This makes the barrier to initial adoption considerably high - as procuring software in many organizations is a non-trivial process involving security review, compliance, legal and other stakeholders.\n\nI viewed solving the above problems as potential opportunities for commercializing nbdev."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#shifting-focus",
    "href": "blog/posts/nbdev/index.html#shifting-focus",
    "title": "On commercializing nbdev",
    "section": "Shifting Focus",
    "text": "Shifting Focus\nJeremy, Wasim, and I eventually settled on the idea of ‚ÄúWordPress for developers,‚Äù a hosted site allowing people to create and share nbdev projects. We thought this would be an excellent way to get people to try nbdev without installing anything. The idea was to narrow the audience to people interested in hosting projects on a platform that promoted exploration and sharing, similar to Glitch that was as easy to use and pragmatic as Wordpress.\nAround the same time we began discussing hosted tools, the machine learning world experienced a tectonic shift due to the explosion of Generative AI, namely Stable Diffusion. fast.ai, the organization that created nbdev, was also changing its focus. fast.ai‚Äôs prime directive was to make deep learning accessible to as many people as possible, and generative AI was too important to ignore. Accordingly, Jeremy placed his full attention on a Stable Diffusion course.\nThis pivot caused some turbulence as we navigated the different priorities of nbdev, generative AI research, and making money. We eventually settled on offering consulting services for everything related to fast.ai in the form of fast.ai partners, which would allow us to bootstrap ourselves financially and embrace the larger mission of fast.ai (including generative AI and research). Eventually, I found the splintered focus across so many areas to be unproductive1 and decided to step away from everything except consulting to regain my footing.\nSoon after that, ChatGPT emerged onto the scene and caused further shifts in machine learning that were orders of magnitude larger than their text-to-image predecessors. Pretty soon, all of my clients were interested in language models, and I found myself working exclusively on operationalizing them (a skill that I have cultivated by working in machine learning for 20+ years). Additionally, LLMs profoundly changed the nature of software development, especially the kind of software development that nbdev was designed to support2. These factors and those discussed earlier suggested it was a good time to step away from nbdev and focus on other things."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#what-i-learned",
    "href": "blog/posts/nbdev/index.html#what-i-learned",
    "title": "On commercializing nbdev",
    "section": "What I learned",
    "text": "What I learned\nI learned some important lessons during this process:\n\nJust because you love a project and find it useful, that doesn‚Äôt necessarily imply that it‚Äôs ripe for commercialization. I always struggled to gain conviction that there was a good business model for nbdev.3 Instead, I pursued this path because I was drawn to the idea of starting a business with people I really liked. Ultimately, I learned that at least one person needs strong conviction in addition to being excited about the people you are working with - not just one or the other.4 I also learned that it‚Äôs important to be honest with yourself about your (and your team‚Äôs) level of conviction and not try to force something that isn‚Äôt there.\nListen to your instincts. I ignored my instincts on multiple occasions throughout this journey. As I‚Äôve grown older, I‚Äôve learned to make this mistake much less often, but I could have done better here.\nDon‚Äôt be afraid to pivot. I think we avoided unnecessary churn by steering clear of a situation that wasn‚Äôt promising. I‚Äôm much more excited about the work I‚Äôm doing now.5\nOwn your own brand. My professional brand became increasingly tied to fast.ai and my friend Jeremy Howard. I‚Äôm grateful for the growth I‚Äôve experienced under this mentorship ‚Äì but I believe it is important to build your own distinct brand and identity. I discovered it can be challenging to build your own brand when you are working on someone else‚Äôs project6, and is something I struggled with. I‚Äôm looking forward to working on this more."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#future-directions",
    "href": "blog/posts/nbdev/index.html#future-directions",
    "title": "On commercializing nbdev",
    "section": "Future Directions",
    "text": "Future Directions\nI suspect that I‚Äôm not completely finished with nbdev. I may revisit the project or related ideas when the time is right. I‚Äôm excited by the work Posit is doing in the areas of literate and exploratory programming, which include many of the ideas explored in nbdev. Wasim has even joined the team at Posit, so I‚Äôm excited to see what they come up with.7\nRegarding what I‚Äôm working on next ‚Äì I‚Äôll have to save my thoughts on that for another post üòä."
  },
  {
    "objectID": "blog/posts/nbdev/index.html#footnotes",
    "href": "blog/posts/nbdev/index.html#footnotes",
    "title": "On commercializing nbdev",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI burned out several times during this process, but I didn‚Äôt realize why at the time. Not surprisingly, trying to focus on too many things at once was the root cause.‚Ü©Ô∏é\nSee this demo for ideas on how coding with LLMs might look like, especially with notebooks.‚Ü©Ô∏é\nThe problem with the hosted solution is that this is not something I would want to use. I can‚Äôt picture myself trying to host code on something other than GitHub/GitLab.‚Ü©Ô∏é\nWithout shared conviction, there is no glue holding everyone together and people can drift apart.‚Ü©Ô∏é\nI‚Äôll share more about this in a future post.‚Ü©Ô∏é\nI don‚Äôt believe this is always the case, but it can be true depending on the dynamics of the group.‚Ü©Ô∏é\nWe previously partnered with Posit and JJ Allaire and built nbdev on top of Quarto. I‚Äôm currently advising Posit on their product and strategy. They have additional projects on their roadmap that I cannot disclose now.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/secret.html",
    "href": "blog/secret.html",
    "title": "Hamel‚Äôs Blog",
    "section": "",
    "text": "This page is supposed to be secret!\n\n\n\nA listing of all my blog posts can be found here\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOn commercializing nbdev\n\n\n\n\n\n\n\nJupyter\n\n\nnbdev\n\n\n\n\nWhy I decided not to commercialize nbdev.\n\n\n\n\n\n\nMay 30, 2023\n\n\nHamel Husain\n\n\n\n\n\n\n  \n\n\n\n\nWhy Should ML Engineers Learn Kubernetes?\n\n\n\n\n\n\n\nK8s\n\n\n\n\nLearning K8s can give you an unreasonable advantage as an MLE and unblock your team.\n\n\n\n\n\n\nJan 16, 2023\n\n\nHamel Husain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/concurrency.html",
    "href": "notes/concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Understand the world of Python concurrency: threads, processes, coroutines and asynchronous programming with a realistic examples.\nSee this blog article."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html",
    "href": "notes/llm/inference/big_inference.html",
    "title": "vLLM & large models",
    "section": "",
    "text": "Correction\n\n\n\nA previous version of this note suggested that you could run Llama 70b on a single A100. This was incorrect. The Modal container was caching the download of the much smaller 7b model. I have updated the post to reflect this. h/t to Cade Daniel for finding the mistake."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#introduction",
    "href": "notes/llm/inference/big_inference.html#introduction",
    "title": "vLLM & large models",
    "section": "Introduction",
    "text": "Introduction\nLarge models like Llama-2-70b may not fit in a single GPU. I previously profiled the smaller 7b model against various inference tools. When a model is too big to fit on a single GPU, we can use various techniques to split the model across multiple GPUs.\n\nCompute & Reproducibility\nI used Modal Labs for serverless compute. Modal is very economical and built for machine learning use cases. Unlike other clouds, there are plenty of A100s available. They even give you $30 of free credits, which is more than enough to run the experiments in this note. Thanks to Modal, the scripts I reference in this note are reproducible.\nIn this note, I‚Äôm using modal client version: 0.50.2889"
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#distributed-inference-w-vllm",
    "href": "notes/llm/inference/big_inference.html#distributed-inference-w-vllm",
    "title": "vLLM & large models",
    "section": "Distributed Inference w/ vLLM",
    "text": "Distributed Inference w/ vLLM\nvLLM supports tensor parallelism, which you can enable by passing the tensor_parallel_size argument to the LLM constructor.\nI modified this example Modal code for Llama v2 13b to run Llama v2 70b on 4 GPUs with tensor parallelism. Below is a simplified diff with the most important changes:\ndef download_model_to_folder():\n    from huggingface_hub import snapshot_download\n\n    snapshot_download(\n-        \"meta-llama/Llama-2-13b-chat-hf\",\n+        \"meta-llama/Llama-2-70b-chat-hf\",\n        local_dir=\"/model\",\n        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    )\n\nimage = (\n    Image.from_dockerhub(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n    .pip_install(\"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\")\n+    # Pin vLLM to 8/2/2023\n+    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@79af7e96a0e2fc9f340d1939192122c3ae38ff17\")\n-    # Pin vLLM to 07/19/2023\n-    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@bda41c70ddb124134935a90a0d51304d2ac035e8\")\n    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n-    .pip_install(\"hf-transfer~=0.1\")\n+     #Force a rebuild to invalidate the cache (you can remove `force_build=True` after the first time)\n+    .pip_install(\"hf-transfer~=0.1\", force_build=True)\n    .run_function(\n        download_model_to_folder,\n        secret=Secret.from_name(\"huggingface\"),\n        timeout=60 * 20)\n)\n...\n\n-@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n+# You need a minimum of 4 A100s that are the 40GB version\n+@stub.cls(gpu=gpu.A100(count=4, memory=40), secret=Secret.from_name(\"huggingface\"))\nclass Model:\n    def __enter__(self):\n        from vllm import LLM\n\n        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n-       self.llm = LLM(MODEL_DIR)\n+       self.llm = LLM(MODEL_DIR, tensor_parallel_size=4)\n...  \nSee big-inference-vllm.py for the actual script I used.\n\n\n\n\n\n\nBe Careful To Mind The Cache When Downloading Files\n\n\n\nI found that when I ran the above code and changed the model name, I had to force a rebuild of the image to invalidate the cache. Otherwise, the old version of the model would be used. You can force a rebuild by adding force_build=True to the .pip_install call.\nWhen I initially wrote this note, I was fooled into believing I could load meta-llama/Llama-2-70b-chat-hf on a single A100. It was this tricky issue of the container that cached the download of the much smaller 7b model. ü§¶\n\n\nAfter setting the appropriate secrets for HuggingFace and Weights & Biases, You can run this code on Modal with the following command:\nmodal run big-inference-vllm.py\nYou need at least 4 A100 GPUs to serve Llama v2 70b."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#what-happens-with-smaller-models",
    "href": "notes/llm/inference/big_inference.html#what-happens-with-smaller-models",
    "title": "vLLM & large models",
    "section": "What Happens With Smaller Models?",
    "text": "What Happens With Smaller Models?\nEven though distributed inference is interesting for big models that do not fit on a single GPU, interesting things happen when you serve smaller models this way. Below, I test throughput for Llama v2 7b on 1, 2, and 4 GPUs. The throughput is measured by passsing these 59 prompts to llm.generate. llm.generate is described in the vLLM documentation:\n\nCall llm.generate to generate the outputs. It adds the input prompts to vLLM engine‚Äôs waiting queue and executes the vLLM engine to generate the outputs with high throughput.\n\nHere are the results, averaged over 5 runs for each row:\nYou can see all the individual runs here. In my experiments, the 70b model needed a minimum of 4 A100s to run, so that‚Äôs why there is only one row for that model (Modal only has instances with 1, 2, or 4 GPUs).\n\n\n\n\n\n\nDo Not Compare To Latency Benchmark\n\n\n\nThe tok/sec number you see here is VERY different than the latency benchmark shown on this note. This particular benchmark maximizes throughput by running multiple requests in parallel. The previous latency benchmark measures the time it takes to process a single request.\n\n\n\nObservations\n\nA100s are much faster than A10s, but A10s are significantly cheaper.1\nOn A10s, scaling up to more GPUs increases throughput at first, but then seems to diminish. It appears like there is a Goldilocks zone in terms of the right number of GPUs to maximize throughput. I did not explore this in detail, as Modal only has instances with specific numbers of GPUs.2\nThe much larger Llama v2 70b model is only ~2x slower than its 7b counterpart."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#aside-pipeline-parallelism",
    "href": "notes/llm/inference/big_inference.html#aside-pipeline-parallelism",
    "title": "vLLM & large models",
    "section": "Aside: Pipeline Parallelism",
    "text": "Aside: Pipeline Parallelism\nIn theory, Pipeline Parallelism (‚ÄúPP‚Äù) is slower than Tensor Parallelism, but tools for PP are compatible with a wider range of models from the HuggingFace Hub. By default, HuggingFace accelerate will automatically split the model across multiple GPUs when you pass device_map=\"auto\". (Accelerate offers other kinds of parallelism as well, like integrations with DeepSpeed).\nThis blog post and these docs are an excellent place to start. I will explore this and other kinds of parallelism in future notes."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#footnotes",
    "href": "notes/llm/inference/big_inference.html#footnotes",
    "title": "vLLM & large models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of 8/6/2023 2 A10s costs .000612 / sec on Modal, whereas 1 A100 40GB will cost 0.001036 / sec. See this pricing chart‚Ü©Ô∏é\nFor A10 and A100s you can only get up to 4 GPUs. Furthermore, I ran into an issue with vLLM and llama 70b, where it doesn‚Äôt like an odd number of GPUs.‚Ü©Ô∏é"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#davinci-resolve",
    "href": "notes/video_editing.html#davinci-resolve",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc\nEpidemic Sound - Sound by mood (Sanyam)\nCayla - Artlist\nCayla - Premium Beat\n\nCayla recommmends 1080p / 24 FPS for Youtube"
  }
]