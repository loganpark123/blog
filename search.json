[
  {
    "objectID": "oss/opensource.html",
    "href": "oss/opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "My open soruce work has been focused on developer tools and infrastructure. I‚Äôve contributed to projects such as fastai, Metaflow, Kubeflow, Jupyter, and Great Expectations, as well as many others. I list some of these below:"
  },
  {
    "objectID": "oss/opensource.html#fastai",
    "href": "oss/opensource.html#fastai",
    "title": " Open Source",
    "section": " fastai",
    "text": "fastai\nI maintain and contribute to a variety of fastai projects. Below are the projects I‚Äôve been very involved in:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nfastpages \n\n\nAn easy to use blogging platform for Jupyter Notebooks. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nnbdev \n\n\nWrite, test, document, and distribute software packages and technical articles all in one place, your notebook. \n\n\nCore Contributor\n\n\nBlog, Talk\n\n\n\n\nfastcore \n\n\nA Python language extension for exploratory and literate programming. \n\n\nCore Contributor\n\n\nBlog\n\n\n\n\nghapi \n\n\nA Python client for the GitHub API \n\n\nCore Contributor\n\n\n Blog\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#metaflow",
    "href": "oss/opensource.html#metaflow",
    "title": " Open Source",
    "section": " Metaflow",
    "text": "Metaflow\nI created notebook cards: A tool that allows you to use notebooks to generate reports, visualizations and diagnostics in Metaflow production workflows. Blog"
  },
  {
    "objectID": "oss/opensource.html#kubeflow",
    "href": "oss/opensource.html#kubeflow",
    "title": " Open Source",
    "section": " Kubeflow",
    "text": "Kubeflow\nI‚Äôve worked on several projects related to Kubeflow, mainly around examples and documentation:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nGitHub Issue Summarization\n\n\nAn end-to-end example of using Kubeflow to summarize GitHub Issues. Became one of the most popular tutorials of Kubeflow. \n\n\nAuthor\n\n\nInterview with Jeremy Lewi\n\n\n\n\nkubeflow/codei-intelligence\n\n\nVarious tutorials and applied examples of Kubeflow. \n\n\nCore Contributor\n\n\nTalk\n\n\n\n\nThe Kubeflow Blog\n\n\nI used fastpages to create the official Kubeflow blog. \n\n\nCore Contributor\n\n\nSite\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#jupyter",
    "href": "oss/opensource.html#jupyter",
    "title": " Open Source",
    "section": " Jupyter",
    "text": "Jupyter\nI created the Repo2Docker GitHub Action, which allows you to trigger repo2docker to build a Jupyter enabled Docker images from your GitHub repository. This Action allows you to pre-cache images for your own BinderHub cluster or for mybinder.org.\nThis project was accepted into the official JupyterHub GitHub org."
  },
  {
    "objectID": "oss/opensource.html#great-expectations",
    "href": "oss/opensource.html#great-expectations",
    "title": " Open Source",
    "section": " Great Expectations",
    "text": "Great Expectations\nI developed the Great Expectations GitHub Action that allows you to use Great Expectations in CI/CD Workflows. Blog."
  },
  {
    "objectID": "oss/opensource.html#other",
    "href": "oss/opensource.html#other",
    "title": " Open Source",
    "section": " Other",
    "text": "Other\nI worked as a staff machine learning engineer at GitHub from 2017 - 2022. I led or created the following open source projects that explored the intersection of machine learning, data and the developer workflow:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nCode Search Net \n\n\nDatasets, tools, and benchmarks for representation learning of code. This was a big part of the inspiration for GitHub‚Äôs eventual work on CoPilot. \n\n\nLead\n\n\n Blog, Paper\n\n\n\n\nMachine Learning Ops\n\n\nA collection of resources on how to facilitate Machine Learning Ops with GitHub. This project explored integrations with a wide variety of data science tools with GitHub Actions. \n\n\nCreator\n\n\nBlog\n\n\n\n\nIssue Label Bot\n\n\nA GitHub App powered by machine learning that auto-labels issues. \n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nCovid19-dashboard \n\n\nA demonstration of how to use GitHub Actions, Jupyter Notebooks and fastpages to create interactive dashboards that update daily.\n\n\n\nCreator\n\n\nNews Article\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/cuda.html",
    "href": "notes/cuda.html",
    "title": "CUDA Version Management",
    "section": "",
    "text": "There are many libraries that only support specific versions of CUDA. Downgrading/upgrading CUDA can sometimes be tricky (especially downgrading). It‚Äôs often desirable to manage CUDA versions per project (instead of globally), without having to reach for Docker."
  },
  {
    "objectID": "notes/cuda.html#problem",
    "href": "notes/cuda.html#problem",
    "title": "CUDA Version Management",
    "section": "",
    "text": "There are many libraries that only support specific versions of CUDA. Downgrading/upgrading CUDA can sometimes be tricky (especially downgrading). It‚Äôs often desirable to manage CUDA versions per project (instead of globally), without having to reach for Docker."
  },
  {
    "objectID": "notes/cuda.html#solution",
    "href": "notes/cuda.html#solution",
    "title": "CUDA Version Management",
    "section": "Solution",
    "text": "Solution\nYou can use conda to manage your CUDA versions! This allows you to isolate specific CUDA versions to specific environments rather than managing CUDA versions globally.\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm using mamba which has faster solvers than conda. Refer to the docs for installation instructions.\n\n\nLet‚Äôs say I want to downgrade to CUDA 11.7 in its own conda environment. First, I will create a new environment named cuda11-7 with the following command:\nmamba create -n cuda11-7 python=3.8\nmamba activate cuda11-7\nBefore I downgrade, we can check our CUDA version with the following command:\n&gt; nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nAs you can see, I have CUDA version 11.8 but I want to downgrade to 11.7. We can downgrade CUDA by using cuda-toolkit:\nmamba install -c \"nvidia/label/cuda-11.7.1\" cuda-toolkit\nThis will take several minutes to complete. Next, recheck your CUDA version:\n&gt; nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\nCuda compilation tools, release 11.7, V11.7.99\nBuild cuda_11.7.r11.7/compiler.31442593_0\nNext, you need to install the correct version of PyTorch for your CUDA version. It is crucial to install the right version of PyTorch that matches your CUDA version exactly. For example, if you want to install PyTorch with CUDA 11.7, you can use the following command:\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\nYou can find PyTorch installation instructions on this page.\nViola! You have downgraded your CUDA version successfully. Note that this version of CUDA is isolated to this specific environment.\nTo make sure that everything is working correctly, make sure you can import torch and check the CUDA version from within Python:\n&gt; python -c \"import torch; print(torch.version.cuda)\"\n11.7"
  },
  {
    "objectID": "notes/cuda.html#additional-resources",
    "href": "notes/cuda.html#additional-resources",
    "title": "CUDA Version Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhy does nvcc --version sometimes report a different CUDA version than nvidia-smi? See this answer on Stack Overflow.\nTwitter discussion on this topic."
  },
  {
    "objectID": "guest-blog.html",
    "href": "guest-blog.html",
    "title": "Guest Blogs",
    "section": "",
    "text": "nbdev + Quarto: A new secret weapon for productivity, the fastai blog, July 2022.\nNotebooks in production with Metaflow Introduces a new Metaflow feature that allows users to use notebooks in production ML workflows.\nPython Concurrency: The Tricky Bits: An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each.\nghapi, a new third-party Python client for the GitHub API by Jeremy Howard & Hamel Husain, GitHub Repo.\nNbdev: A literate programming environment that democratizes software engineering best practices by Hamel Husain, Jeremy Howard, The GitHub Blog.\nfastcore: An Underrated Python Library by Hamel Husain, Jeremy Howard, GitHub Repo.\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes: An end-to-end example of deploying a machine learning product using Jupyter, Papermill, Tekton, GitOps and Kubeflow. by Jeremy Lewi, Hamel_Husain, The Kubeflow Blog.\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. by Jeremy Howard & Hamel Husain, GitHub Repo\nGitHub Actions: Providing Data Scientists With New Superpowers by Jeremy Howard & Hamel Husain.\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search: by Miltiadis Allamanis, Marc Brockschmidt, Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit GitHub Repo\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning. (Related: GitHub engineering blog article, Live demo)\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\nHow Docker Can Make You A More Effective Data Scientist\nAutomated Machine Learning, A Paradigm Shift That Accelerates Data Scientst Productivity At Airbnb"
  },
  {
    "objectID": "blog/posts/arhome/index.html",
    "href": "blog/posts/arhome/index.html",
    "title": "ARHome",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "üé§ Talks",
    "section": "",
    "text": "These are a list of talks I‚Äôve given:\n\nInnovating on Software Development, Data Council, March 2023.\nAutoML, Literate Programming, and Data Tooling Cargo Cults, Vanishing Gradients Podcast with Hugo Bowne Anderson, July 2022."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logans's Blog",
    "section": "",
    "text": "Hello, I‚Äôm Logan Parker. I‚Äôm a machine learning engineer who loves building machine learning infrastructure and tools üë∑üèº‚Äç‚ôÇÔ∏è. I currently work at CoreLogic primarily focusing on Computer Vision (CV) and 3D reconstruction. Furthermore, I have experience (2+ years) as a software engineer across various industries.\nI‚Äôm currently exploring entrepreneurship at the intersection of dev tools, ML platforms and large language models. I am working on some projects right now including a proper solution to pesky bill splitting."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Logans's Blog",
    "section": "üíº Get In Touch",
    "text": "üíº Get In Touch\nDo you need help operationalizing ML or large language models or maybe want to chat about VR research?\nEmail me at logansparker@gmail.com if you‚Äôd like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Logans's Blog",
    "section": "üìÆ Blog Posts",
    "text": "üìÆ Blog Posts\nSubscribe here\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n\n\n5/7/22\n\n\nIncreasing Perceived Realism of Objects in a Mixed Reality Environment Using ‚ÄòDiminished Virtual Reality‚Äô\n\n\nUndergraduate Honors Thesis \n\n\nundefined\n\n\n\n\n12/1/21\n\n\nARHome\n\n\nA Captsone Project\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/secret.html",
    "href": "blog/secret.html",
    "title": "Hamel‚Äôs Blog",
    "section": "",
    "text": "This page is supposed to be secret!\n\n\n\nA listing of all my blog posts can be found here\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nARHome\n\n\n\n\n\nA Captsone Project\n\n\n\n\n\n\nDec 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/concurrency.html",
    "href": "notes/concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Understand the world of Python concurrency: threads, processes, coroutines and asynchronous programming with a realistic examples.\nSee this blog article."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html",
    "href": "notes/llm/inference/big_inference.html",
    "title": "vLLM & large models",
    "section": "",
    "text": "Correction\n\n\n\nA previous version of this note suggested that you could run Llama 70b on a single A100. This was incorrect. The Modal container was caching the download of the much smaller 7b model. I have updated the post to reflect this. h/t to Cade Daniel for finding the mistake."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#introduction",
    "href": "notes/llm/inference/big_inference.html#introduction",
    "title": "vLLM & large models",
    "section": "Introduction",
    "text": "Introduction\nLarge models like Llama-2-70b may not fit in a single GPU. I previously profiled the smaller 7b model against various inference tools. When a model is too big to fit on a single GPU, we can use various techniques to split the model across multiple GPUs.\n\nCompute & Reproducibility\nI used Modal Labs for serverless compute. Modal is very economical and built for machine learning use cases. Unlike other clouds, there are plenty of A100s available. They even give you $30 of free credits, which is more than enough to run the experiments in this note. Thanks to Modal, the scripts I reference in this note are reproducible.\nIn this note, I‚Äôm using modal client version: 0.50.2889"
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#distributed-inference-w-vllm",
    "href": "notes/llm/inference/big_inference.html#distributed-inference-w-vllm",
    "title": "vLLM & large models",
    "section": "Distributed Inference w/ vLLM",
    "text": "Distributed Inference w/ vLLM\nvLLM supports tensor parallelism, which you can enable by passing the tensor_parallel_size argument to the LLM constructor.\nI modified this example Modal code for Llama v2 13b to run Llama v2 70b on 4 GPUs with tensor parallelism. Below is a simplified diff with the most important changes:\ndef download_model_to_folder():\n    from huggingface_hub import snapshot_download\n\n    snapshot_download(\n-        \"meta-llama/Llama-2-13b-chat-hf\",\n+        \"meta-llama/Llama-2-70b-chat-hf\",\n        local_dir=\"/model\",\n        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n    )\n\nimage = (\n    Image.from_dockerhub(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n    .pip_install(\"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\")\n+    # Pin vLLM to 8/2/2023\n+    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@79af7e96a0e2fc9f340d1939192122c3ae38ff17\")\n-    # Pin vLLM to 07/19/2023\n-    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@bda41c70ddb124134935a90a0d51304d2ac035e8\")\n    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n-    .pip_install(\"hf-transfer~=0.1\")\n+     #Force a rebuild to invalidate the cache (you can remove `force_build=True` after the first time)\n+    .pip_install(\"hf-transfer~=0.1\", force_build=True)\n    .run_function(\n        download_model_to_folder,\n        secret=Secret.from_name(\"huggingface\"),\n        timeout=60 * 20)\n)\n...\n\n-@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n+# You need a minimum of 4 A100s that are the 40GB version\n+@stub.cls(gpu=gpu.A100(count=4, memory=40), secret=Secret.from_name(\"huggingface\"))\nclass Model:\n    def __enter__(self):\n        from vllm import LLM\n\n        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n-       self.llm = LLM(MODEL_DIR)\n+       self.llm = LLM(MODEL_DIR, tensor_parallel_size=4)\n...  \nSee big-inference-vllm.py for the actual script I used.\n\n\n\n\n\n\nBe Careful To Mind The Cache When Downloading Files\n\n\n\nI found that when I ran the above code and changed the model name, I had to force a rebuild of the image to invalidate the cache. Otherwise, the old version of the model would be used. You can force a rebuild by adding force_build=True to the .pip_install call.\nWhen I initially wrote this note, I was fooled into believing I could load meta-llama/Llama-2-70b-chat-hf on a single A100. It was this tricky issue of the container that cached the download of the much smaller 7b model. ü§¶\n\n\nAfter setting the appropriate secrets for HuggingFace and Weights & Biases, You can run this code on Modal with the following command:\nmodal run big-inference-vllm.py\nYou need at least 4 A100 GPUs to serve Llama v2 70b."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#what-happens-with-smaller-models",
    "href": "notes/llm/inference/big_inference.html#what-happens-with-smaller-models",
    "title": "vLLM & large models",
    "section": "What Happens With Smaller Models?",
    "text": "What Happens With Smaller Models?\nEven though distributed inference is interesting for big models that do not fit on a single GPU, interesting things happen when you serve smaller models this way. Below, I test throughput for Llama v2 7b on 1, 2, and 4 GPUs. The throughput is measured by passsing these 59 prompts to llm.generate. llm.generate is described in the vLLM documentation:\n\nCall llm.generate to generate the outputs. It adds the input prompts to vLLM engine‚Äôs waiting queue and executes the vLLM engine to generate the outputs with high throughput.\n\nHere are the results, averaged over 5 runs for each row:\nYou can see all the individual runs here. In my experiments, the 70b model needed a minimum of 4 A100s to run, so that‚Äôs why there is only one row for that model (Modal only has instances with 1, 2, or 4 GPUs).\n\n\n\n\n\n\nDo Not Compare To Latency Benchmark\n\n\n\nThe tok/sec number you see here is VERY different than the latency benchmark shown on this note. This particular benchmark maximizes throughput by running multiple requests in parallel. The previous latency benchmark measures the time it takes to process a single request.\n\n\n\nObservations\n\nA100s are much faster than A10s, but A10s are significantly cheaper.1\nOn A10s, scaling up to more GPUs increases throughput at first, but then seems to diminish. It appears like there is a Goldilocks zone in terms of the right number of GPUs to maximize throughput. I did not explore this in detail, as Modal only has instances with specific numbers of GPUs.2\nThe much larger Llama v2 70b model is only ~2x slower than its 7b counterpart."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#aside-pipeline-parallelism",
    "href": "notes/llm/inference/big_inference.html#aside-pipeline-parallelism",
    "title": "vLLM & large models",
    "section": "Aside: Pipeline Parallelism",
    "text": "Aside: Pipeline Parallelism\nIn theory, Pipeline Parallelism (‚ÄúPP‚Äù) is slower than Tensor Parallelism, but tools for PP are compatible with a wider range of models from the HuggingFace Hub. By default, HuggingFace accelerate will automatically split the model across multiple GPUs when you pass device_map=\"auto\". (Accelerate offers other kinds of parallelism as well, like integrations with DeepSpeed).\nThis blog post and these docs are an excellent place to start. I will explore this and other kinds of parallelism in future notes."
  },
  {
    "objectID": "notes/llm/inference/big_inference.html#footnotes",
    "href": "notes/llm/inference/big_inference.html#footnotes",
    "title": "vLLM & large models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of 8/6/2023 2 A10s costs .000612 / sec on Modal, whereas 1 A100 40GB will cost 0.001036 / sec. See this pricing chart‚Ü©Ô∏é\nFor A10 and A100s you can only get up to 4 GPUs. Furthermore, I ran into an issue with vLLM and llama 70b, where it doesn‚Äôt like an odd number of GPUs.‚Ü©Ô∏é"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#davinci-resolve",
    "href": "notes/video_editing.html#davinci-resolve",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -&gt; ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc\nEpidemic Sound - Sound by mood (Sanyam)\nCayla - Artlist\nCayla - Premium Beat\n\nCayla recommmends 1080p / 24 FPS for Youtube"
  }
]